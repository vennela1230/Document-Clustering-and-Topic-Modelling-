{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njRdJ8KQijf3"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install -q sentence-transformers scikit-learn pandas nltk faiss-cpu torch gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1) Imports\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "miFgGoefinvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2) Load dataset and preprocess\n",
        "newsgroups = fetch_20newsgroups(subset='all', remove=('headers','footers','quotes'))\n",
        "docs = newsgroups.data\n",
        "labels = newsgroups.target\n",
        "target_names = newsgroups.target_names\n",
        "\n",
        "df = pd.DataFrame({'document': docs, 'label': labels})\n",
        "df['document'] = df['document'].str.replace('\\n', ' ').str.strip()\n",
        "df = df[df['document'].str.len() > 20].reset_index(drop=True)\n",
        "print(\"Loaded dataset, documents:\", len(df))"
      ],
      "metadata": {
        "id": "-rKbD3nElFok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 3) Load SentenceTransformer model (safe)\n",
        "from sentence_transformers import SentenceTransformer\n",
        "print(\"Loading SentenceTransformer (all-MiniLM-L6-v2). This may take a moment...\")\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n",
        "print(\"Model loaded.\")"
      ],
      "metadata": {
        "id": "OgWiuy6hlG3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Generate BERT embeddings for all documents (safe batching)\n",
        "documents = df['document'].tolist()\n",
        "embeddings = model.encode(documents, batch_size=64, show_progress_bar=True, convert_to_numpy=True)\n",
        "print(\"Embeddings shape:\", embeddings.shape)"
      ],
      "metadata": {
        "id": "THysdTJklKlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) KMeans clustering\n",
        "from sklearn.cluster import KMeans\n",
        "num_clusters = 20\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
        "df['cluster'] = kmeans.fit_predict(embeddings)\n",
        "print(\"KMeans clustering done. Cluster counts:\")\n",
        "print(df['cluster'].value_counts().sort_index())"
      ],
      "metadata": {
        "id": "olja60dYlRsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# AUTO-LABEL ALL 20 CLUSTERS\n",
        "# ==========================================\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "# Real-world categories you want to assign\n",
        "categories = [\n",
        "    \"sports\",\n",
        "    \"politics\",\n",
        "    \"technology\",\n",
        "    \"religion\",\n",
        "    \"business\",\n",
        "    \"health\",\n",
        "    \"education\",\n",
        "    \"science\",\n",
        "    \"entertainment\",\n",
        "    \"crime\",\n",
        "    \"travel\",\n",
        "    \"law\",\n",
        "    \"finance\",\n",
        "    \"history\"\n",
        "]\n",
        "\n",
        "# Load embedding model\n",
        "label_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Pre-compute category embeddings\n",
        "category_embs = label_model.encode(categories, convert_to_tensor=True)\n",
        "\n",
        "cluster_real_labels = {}\n",
        "\n",
        "for cluster_id in sorted(cluster_topics.keys()):\n",
        "\n",
        "    keywords = cluster_topics[cluster_id]\n",
        "    keyword_text = \" \".join(keywords)\n",
        "\n",
        "    # Embed cluster keywords\n",
        "    key_emb = label_model.encode(keyword_text, convert_to_tensor=True)\n",
        "\n",
        "    # Compare with each category\n",
        "    sim_scores = util.cos_sim(key_emb, category_embs)[0]\n",
        "\n",
        "    # Best matching category\n",
        "    best_idx = torch.argmax(sim_scores).item()\n",
        "    best_label = categories[best_idx]\n",
        "    best_score = sim_scores[best_idx].item()\n",
        "\n",
        "    cluster_real_labels[cluster_id] = (best_label, best_score)\n",
        "\n",
        "# ==========================\n",
        "# PRINT FINAL RESULTS\n",
        "# ==========================\n",
        "print(\"\\n==============================\")\n",
        "print(\" AUTO LABELS FOR ALL 20 CLUSTERS \")\n",
        "print(\"==============================\\n\")\n",
        "\n",
        "for cid, (label, score) in cluster_real_labels.items():\n",
        "    print(f\"Cluster {cid}:  {label.upper()}   (confidence={score:.3f})\")\n"
      ],
      "metadata": {
        "id": "nJdW5Tftn8Ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 6) Topic modelling per cluster (TF-IDF top keywords)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "def extract_cluster_topics(df, num_keywords=10):\n",
        "    cluster_topics = {}\n",
        "    for cid in sorted(df['cluster'].unique()):\n",
        "        docs_cluster = df[df['cluster']==cid]['document'].values\n",
        "        if len(docs_cluster) == 0:\n",
        "            cluster_topics[cid] = []\n",
        "            continue\n",
        "        vect = TfidfVectorizer(stop_words='english', max_features=2000)\n",
        "        X = vect.fit_transform(docs_cluster)\n",
        "        avg = X.mean(axis=0).A1\n",
        "        terms = vect.get_feature_names_out()\n",
        "        top_idx = avg.argsort()[::-1][:num_keywords]\n",
        "        cluster_topics[cid] = [terms[i] for i in top_idx]\n",
        "    return cluster_topics\n",
        "\n",
        "cluster_topics = extract_cluster_topics(df, num_keywords=10)\n",
        "print(\"\\nSample cluster topics (first 5 clusters):\")\n",
        "for i in range(5):\n",
        "    print(f\"Cluster {i}: {', '.join(cluster_topics.get(i,[]))}\")"
      ],
      "metadata": {
        "id": "gTA7_KsmlWUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 7) Build FAISS index for global semantic search (cosine via normalized inner product)\n",
        "import faiss\n",
        "embed_matrix = embeddings.astype('float32')\n",
        "faiss.normalize_L2(embed_matrix)   # normalize for cosine similarity\n",
        "d = embed_matrix.shape[1]\n",
        "index = faiss.IndexFlatIP(d)\n",
        "index.add(embed_matrix)\n",
        "print(\"\\nFAISS index created with {} vectors (dim={}).\".format(index.ntotal, d))"
      ],
      "metadata": {
        "id": "SqSQ7eqAldvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8) Train a small MLP classifier (PyTorch) on embeddings -> cluster labels (for confidence)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = embeddings.astype(np.float32)\n",
        "y = df['cluster'].values\n",
        "\n",
        "# train/val split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.12, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "X_train_t = torch.tensor(X_train).to(device)\n",
        "X_val_t = torch.tensor(X_val).to(device)\n",
        "\n",
        "# FIX: convert labels to LONG tensors\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.long).to(device)\n",
        "y_val_t = torch.tensor(y_val, dtype=torch.long).to(device)\n",
        "\n",
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "input_dim = X.shape[1]\n",
        "num_classes = len(np.unique(y))\n",
        "hidden_dim = 512\n",
        "\n",
        "mlp = MLPClassifier(input_dim, hidden_dim, num_classes).to(device)\n",
        "optimizer = optim.Adam(mlp.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "epochs = 12\n",
        "batch_size = 128\n",
        "\n",
        "print(\"\\nTraining MLP classifier (fixed version)...\")\n",
        "\n",
        "mlp.train()\n",
        "n = X_train_t.shape[0]\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    perm = torch.randperm(n)\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for i in range(0, n, batch_size):\n",
        "        idx = perm[i:i+batch_size]\n",
        "        xb = X_train_t[idx]\n",
        "        yb = y_train_t[idx]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = mlp(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item() * xb.size(0)\n",
        "\n",
        "    mlp.eval()\n",
        "    with torch.no_grad():\n",
        "        val_logits = mlp(X_val_t)\n",
        "        val_preds = val_logits.argmax(dim=1)\n",
        "        val_acc = (val_preds == y_val_t).float().mean().item()\n",
        "\n",
        "    mlp.train()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}   Loss: {epoch_loss/n:.4f}   Val Acc: {val_acc:.4f}\")"
      ],
      "metadata": {
        "id": "Qo8ltx7clo2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 9) Optional: Train Doc2Vec (gensim) to add deep-learning doc similarity\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "import gensim\n",
        "tagged = [TaggedDocument(words=doc.split(), tags=[i]) for i, doc in enumerate(df['document'])]\n",
        "doc2vec_model = Doc2Vec(vector_size=100, min_count=2, epochs=30, workers=4)\n",
        "doc2vec_model.build_vocab(tagged)\n",
        "doc2vec_model.train(tagged, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n",
        "print(\"Doc2Vec trained.\")"
      ],
      "metadata": {
        "id": "L7Ab39nPlwzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 10) Helper search functions\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# FAISS search\n",
        "def faiss_search(query, top_k=5):\n",
        "    q_emb = model.encode([query], convert_to_numpy=True).astype('float32')\n",
        "    faiss.normalize_L2(q_emb)\n",
        "    scores, idx = index.search(q_emb, top_k)\n",
        "    return scores[0], idx[0]\n",
        "\n",
        "# NN + similarity in-cluster search (confidence + cosine)\n",
        "prob_threshold = 0.55  # tuneable\n",
        "\n",
        "def nn_cluster_search(query, top_k=5, prob_threshold=prob_threshold):\n",
        "    q_emb = model.encode([query], convert_to_numpy=True).astype(np.float32)\n",
        "    q_tensor = torch.tensor(q_emb).to(device)\n",
        "    mlp.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = mlp(q_tensor)\n",
        "        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "    top_prob = probs.max()\n",
        "    pred_cluster = int(probs.argmax())\n",
        "    if top_prob < prob_threshold:\n",
        "        return \"No similar text found\", float(top_prob), None\n",
        "    # gather cluster docs\n",
        "    cluster_idxs = df[df['cluster']==pred_cluster].index.values\n",
        "    if len(cluster_idxs) == 0:\n",
        "        return \"No similar text found\", float(top_prob), None\n",
        "    cluster_embs = X[cluster_idxs]\n",
        "    sims = cosine_similarity(q_emb, cluster_embs)[0]\n",
        "    top_local = sims.argsort()[::-1][:top_k]\n",
        "    top_doc_idxs = cluster_idxs[top_local]\n",
        "    docs_scores = [(df.loc[i,'document'], float(sims[j])) for j,i in enumerate(top_doc_idxs)]\n",
        "    return pred_cluster, float(top_prob), docs_scores\n",
        "\n",
        "# Doc2Vec search\n",
        "def doc2vec_search(query, top_k=5):\n",
        "    q_vec = doc2vec_model.infer_vector(query.split())\n",
        "    sims = doc2vec_model.dv.most_similar([q_vec], topn=top_k)\n",
        "    results = [(df.loc[int(doc_id),'document'], float(score)) for doc_id, score in sims]\n",
        "    return results"
      ],
      "metadata": {
        "id": "bZAunoXRl3cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11) Interactive demo loop: choose search mode\n",
        "print(\"\\nInteractive demo: Choose search mode:\")\n",
        "print(\" - 'nn_similarity' : MLP confidence + in-cluster cosine similarity (recommended)\")\n",
        "print(\" - 'faiss'         : Global FAISS semantic search (fast & global)\")\n",
        "print(\" - 'doc2vec'       : Doc2Vec-based similarity\")\n",
        "print(\"Type 'exit' to quit.\\n\")\n",
        "\n",
        "while True:\n",
        "    mode = input(\"Choose mode (nn_similarity/faiss/doc2vec) or 'exit': \").strip().lower()\n",
        "    if mode == 'exit':\n",
        "        break\n",
        "    if mode not in {'nn_similarity','faiss','doc2vec'}:\n",
        "        print(\"Invalid mode. Try again.\")\n",
        "        continue\n",
        "    query = input(\"\\nEnter document text (paste full paragraph recommended):\\n\")\n",
        "    if query.strip().lower() == 'exit':\n",
        "        break\n",
        "    if mode == 'faiss':\n",
        "        scores, idxs = faiss_search(query, top_k=5)\n",
        "        print(\"\\nTop FAISS matches (score, snippet):\")\n",
        "        for s,i in zip(scores, idxs):\n",
        "            print(f\"\\nScore: {s:.4f}\\n{df.loc[i,'document'][:400]}...\\n\")\n",
        "    elif mode == 'nn_similarity':\n",
        "        cluster_or_msg, conf, docs_scores = nn_cluster_search(query, top_k=5)\n",
        "        if cluster_or_msg == \"No similar text found\":\n",
        "            print(f\"\\nNo similar text found (NN confidence={conf:.3f})\\n\")\n",
        "        else:\n",
        "            print(f\"\\nPredicted cluster: {cluster_or_msg}  (NN confidence={conf:.3f})\")\n",
        "            if cluster_or_msg in cluster_topics:\n",
        "                print(\"Cluster topics:\", \", \".join(cluster_topics[cluster_or_msg]))\n",
        "            print(\"\\nTop documents in cluster (score, snippet):\")\n",
        "            for doc, sc in docs_scores:\n",
        "                print(f\"\\nScore: {sc:.4f}\\n{doc[:400]}...\\n\")\n",
        "    else:  # doc2vec\n",
        "        res = doc2vec_search(query, top_k=5)\n",
        "        print(\"\\nDoc2Vec matches (score, snippet):\")\n",
        "        for doc, sc in res:\n",
        "            print(f\"\\nScore: {sc:.4f}\\n{doc[:400]}...\\n\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Optionally save trained MLP and Doc2Vec\n",
        "torch.save(mlp.state_dict(), \"mlp_cluster_classifier.pt\")\n",
        "doc2vec_model.save(\"doc2vec_model.model\")\n",
        "print(\"Models saved: mlp_cluster_classifier.pt , doc2vec_model.model\")"
      ],
      "metadata": {
        "id": "yNFKi7vil8dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11) Interactive demo loop: choose search mode\n",
        "print(\"\\nInteractive demo: Choose search mode:\")\n",
        "print(\" - 'nn_similarity' : MLP confidence + in-cluster cosine similarity (recommended)\")\n",
        "print(\" - 'faiss'         : Global FAISS semantic search (fast & global)\")\n",
        "print(\" - 'doc2vec'       : Doc2Vec-based similarity\")\n",
        "print(\"Type 'exit' to quit.\\n\")\n",
        "\n",
        "while True:\n",
        "    mode = input(\"Choose mode (nn_similarity/faiss/doc2vec) or 'exit': \").strip().lower()\n",
        "    if mode == 'exit':\n",
        "        break\n",
        "    if mode not in {'nn_similarity','faiss','doc2vec'}:\n",
        "        print(\"Invalid mode. Try again.\")\n",
        "        continue\n",
        "\n",
        "    query = input(\"\\nEnter document text (paste full paragraph recommended):\\n\")\n",
        "    if query.strip().lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    # ---------------------------\n",
        "    # 1) FAISS MODE\n",
        "    # ---------------------------\n",
        "    if mode == 'faiss':\n",
        "        scores, idxs = faiss_search(query, top_k=5)\n",
        "        print(\"\\nTop FAISS matches (score, cluster, snippet):\")\n",
        "        for s, i in zip(scores, idxs):\n",
        "            cluster_id = df.loc[i, 'cluster']\n",
        "            snippet = df.loc[i, 'document'][:400]\n",
        "            print(f\"\\nScore: {s:.4f} | Cluster: {cluster_id}\\n{snippet}...\\n\")\n",
        "\n",
        "    # ---------------------------\n",
        "    # 2) NEURAL-NET + COSINE\n",
        "    # ---------------------------\n",
        "    elif mode == 'nn_similarity':\n",
        "        cluster_or_msg, conf, docs_scores = nn_cluster_search(query, top_k=5)\n",
        "\n",
        "        if cluster_or_msg == \"No similar text found\":\n",
        "            print(f\"\\nNo similar text found (NN confidence={conf:.3f})\\n\")\n",
        "        else:\n",
        "            print(f\"\\nPredicted cluster: {cluster_or_msg}  (NN confidence={conf:.3f})\")\n",
        "\n",
        "            if cluster_or_msg in cluster_topics:\n",
        "                print(\"Cluster topics:\", \", \".join(cluster_topics[cluster_or_msg]))\n",
        "\n",
        "            print(\"\\nTop documents in this cluster (score, snippet):\")\n",
        "            for doc, sc in docs_scores:\n",
        "                print(f\"\\nScore: {sc:.4f}\\n{doc[:400]}...\\n\")\n",
        "\n",
        "    # ---------------------------\n",
        "    # 3) DOC2VEC MODE\n",
        "    # ---------------------------\n",
        "    else:  # doc2vec\n",
        "        res = doc2vec_search(query, top_k=5)\n",
        "        print(\"\\nDoc2Vec matches (score, cluster, snippet):\")\n",
        "        for doc, sc in res:\n",
        "            # Find original index & cluster\n",
        "            idx = df.index[df['document'] == doc][0]\n",
        "            cluster_id = df.loc[idx, 'cluster']\n",
        "            print(f\"\\nScore: {sc:.4f} | Cluster: {cluster_id}\\n{doc[:400]}...\\n\")\n",
        "\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# save models\n",
        "torch.save(mlp.state_dict(), \"mlp_cluster_classifier.pt\")\n",
        "doc2vec_model.save(\"doc2vec_model.model\")\n",
        "print(\"Models saved: mlp_cluster_classifier.pt , doc2vec_model.model\")"
      ],
      "metadata": {
        "id": "V3-W3DOYwCKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11) Interactive demo loop: choose search mode\n",
        "print(\"\\nInteractive demo: Choose search mode:\")\n",
        "print(\" - 'nn_similarity' : MLP confidence + in-cluster cosine similarity (recommended)\")\n",
        "print(\" - 'faiss'         : Global FAISS semantic search (fast & global)\")\n",
        "print(\" - 'doc2vec'       : Doc2Vec-based similarity\")\n",
        "print(\"Type 'exit' to quit.\\n\")\n",
        "\n",
        "while True:\n",
        "    mode = input(\"Choose mode (nn_similarity/faiss/doc2vec) or 'exit': \").strip().lower()\n",
        "    if mode == 'exit':\n",
        "        break\n",
        "    if mode not in {'nn_similarity', 'faiss', 'doc2vec'}:\n",
        "        print(\"Invalid mode. Try again.\")\n",
        "        continue\n",
        "\n",
        "    query = input(\"\\nEnter document text (paste full paragraph recommended):\\n\")\n",
        "    if query.strip().lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    # Encode query\n",
        "    query_emb = model.encode([query], convert_to_numpy=True)\n",
        "\n",
        "    # Predict main cluster of the query\n",
        "    predicted_cluster = kmeans.predict(query_emb)[0]\n",
        "\n",
        "    # Print main cluster label\n",
        "    print(f\"\\nPredicted Cluster: {predicted_cluster}\")\n",
        "    if predicted_cluster in cluster_topics:\n",
        "        print(\"Cluster Topics:\", \", \".join(cluster_topics[predicted_cluster]))\n",
        "\n",
        "    # ---------------------------\n",
        "    # 1) FAISS MODE (Fixed)\n",
        "    # ---------------------------\n",
        "    if mode == 'faiss':\n",
        "        scores, idxs = faiss_search(query, top_k=5)\n",
        "\n",
        "        print(\"\\nTop FAISS matches (score, snippet):\")\n",
        "        for s, i in zip(scores, idxs):\n",
        "            snippet = df.loc[i, 'document'][:400]\n",
        "            print(f\"\\nScore: {s:.4f}\\n{snippet}...\\n\")\n",
        "\n",
        "    # ---------------------------\n",
        "    # 2) NEURAL-NET + COSINE (Fixed)\n",
        "    # ---------------------------\n",
        "    elif mode == 'nn_similarity':\n",
        "\n",
        "        # Use NN to classify cluster (already shown above)\n",
        "        x_tensor = torch.tensor(query_emb, dtype=torch.float32)\n",
        "        probs = mlp(x_tensor).detach().numpy()\n",
        "        nn_conf = probs[0][predicted_cluster]\n",
        "\n",
        "        # Get documents only from that cluster\n",
        "        cluster_docs = df[df['cluster'] == predicted_cluster]\n",
        "\n",
        "        # Compute cosine similarity\n",
        "        doc_embs = cluster_docs['bert_emb'].to_list()\n",
        "        sim_scores = cosine_similarity(query_emb, np.vstack(doc_embs))[0]\n",
        "\n",
        "        # Top similar docs within cluster\n",
        "        top_idx = sim_scores.argsort()[::-1][:5]\n",
        "\n",
        "        print(f\"\\nNeural Network Confidence: {nn_conf:.4f}\")\n",
        "        print(\"\\nTop documents in this cluster:\")\n",
        "\n",
        "        for idx in top_idx:\n",
        "            doc_text = cluster_docs.iloc[idx]['document'][:400]\n",
        "            score = sim_scores[idx]\n",
        "            print(f\"\\nScore: {score:.4f}\\n{doc_text}...\\n\")\n",
        "\n",
        "    # ---------------------------\n",
        "    # 3) DOC2VEC MODE (Fixed)\n",
        "    # ---------------------------\n",
        "    else:\n",
        "        results = doc2vec_search(query, top_k=5)\n",
        "        print(\"\\nDoc2Vec matches (score, snippet):\")\n",
        "        for doc, sc in results:\n",
        "            print(f\"\\nScore: {sc:.4f}\\n{doc[:400]}...\\n\")\n",
        "\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Save models\n",
        "torch.save(mlp.state_dict(), \"mlp_cluster_classifier.pt\")\n",
        "doc2vec_model.save(\"doc2vec_model.model\")\n",
        "print(\"Models saved: mlp_cluster_classifier.pt , doc2vec_model.model\")\n"
      ],
      "metadata": {
        "id": "Fkv7KeG7wDbW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}